<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Home | IJCAI 2022 Tutorial: Differentiable Optimization: Integrating Structural Information into Training Pipeline</title><meta name="description" content="Page-specific description"><meta name="keywords" content="Page-specific keywords"><link rel="shortcut icon" href="favicon.ico"><link rel="shortcut icon" href="static/images/favicons/favicon.png" type="image/x-icon"><link rel="apple-touch-icon" href="static/images/favicons/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="static/images/favicons/apple-touch-icon-precomposed.png"><link href="https://fonts.googleapis.com/css?family=Lato:400,300,700" rel="stylesheet" type="text/css"><link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css"><link rel="stylesheet" href="static/css/screen.min.css"><link rel="stylesheet" href="static/css/custom.min.css"></head><body><!-- $Page header--><header class="page-header js-header"><div class="page-header__inner"><div class="page-header__logo"><h1>Tutorial</h1>&nbsp;<span>Differentiable Optimization</span></div><nav class="page-header__nav js-nav"><ul><li><a class="page-header__nav-link js-anchor js-link is-active" href="#home">Home</a></li><li><a class="page-header__nav-link js-anchor js-link" href="#portfolio">Tutorial</a></li><li><a class="page-header__nav-link js-anchor js-link" href="#team">Team</a></li><li><a class="page-header__nav-link js-anchor js-link" href="#contact">Contact</a></li></ul></nav></div></header><!-- $Promo section--><section class="promo js-jumbo" id="home"><h1 class="promo__title"><span>IJCAI 2022 Tutorial;</span></h1><h2 class="promo__title"><span>Differentiable Optimization: Integrating Structural Information into Training Pipeline</span></h2><h2 class="promo__subtitle"><span>July 25th 14:00-17:30 @ T29 Lehar 1</span></h2><a class="promo__go-next js-anchor" href="#portfolio">Scroll down</a></section><!-- $Portfolio section--><section class="portfolio js-portfolio" id="portfolio"><div class="portfolio__inner"><h2 class="portfolio__title">Abstract</h2><p class="portfolio__descr">Structural information and domain knowledge are two necessary components of training a good machine learning model to maximize the performance in the targeted application. This tutorial summarizes how to use optimization as a differentiable building block to incorporate the non-trivial operational information in applications into machine learning models.</p><h2 class="portfolio__title">Tutorial Description</h2><p class="portfolio__descr">Machine learning models have achieved significant success in many industrial applications and social challenges, including natural language processing, computer vision, time series analysis, and recommendation systems. To adapt to different applications, incorporating structural information and domain knowledge in applications into machine learning models is an important element of the training process. But it often relies on fine-tuning and feature-engineering without a systematic approach to adapt to various applications. On the other hand, operational research is an application-driven approach, where optimization problems are formulated based on the knowledge and constraints of targeted applications to derive actionable solutions. Optimization formulations can capture structural information and domain knowledge in applications, but the non-differentiability and the complex operational processes in optimization make it hard to integrate into machine learning models.<br><br>This tutorial starts from the foundation of differentiable optimization to discuss how to convert optimization into differentiable building blocks to use in larger architectures. The direct benefit of differentiable optimization is to integrate structural information and domain knowledge in optimization formulations into machine learning models. The first part of the tutorial covers a variety of applications using optimization as differentiable units in machine learning models to properly handle operational tasks in reinforcement learning, control, optimal transport, and geometry. Experiments demonstrate that differentiable optimization can model operational processes more efficiently than neural networks. The second part of the tutorial focuses on integrating various industrial and social challenges as differentiable optimization layers into the training pipeline. This integration of machine learning models and application-driven optimization leads to end-to-end learning, decision-focused learning, that trains models to directly optimize the performance in targeted applications. Lastly, the tutorial concludes with a series of applications of differentiable optimization and its computational limitations with various open directions left to the audiences.</p></div></section><!-- $Team section--><section class="team" id="team"><div class="team__inner"><h2 class="team__title">About us</h2><div class="team__member"><img src="static/images/kai.png" alt="Kai Wang"><div class="team__member-info"><h3 class="team__member-name">Kai Wang</h3><span class="team__member-role">Harvard University</span><p class="team__member-descr">I am a Ph.D. candidate studying Computer Science at Harvard University working with Professor Milind Tambe. My research focuses on using artificial intelligence to resolve various social challenges, including wildlife conservation and healthcare challenges. I formulate these social challenges as multi-agent systems and learn the unknown information from data using machine learning. I apply and generalize a new learning algorithm, decision-focused learning, to integrate the domain knowledge from social challenges into machine learning pipelines.</p></div><ul class="team__member-soc"><li><a class="team__member-icon tw" href="https://twitter.com/kaiwang_gua">Follow me on twitter</a></li><li><a class="team__member-icon gplus" href="https://scholar.google.com/citations?user=gGSsQmsAAAAJ&amp;hl=en">My Google+</a></li><li><a class="team__member-icon db" href="https://guaguakai.github.io/">Personal website</a></li></ul></div><div class="team__member"><img src="static/images/andrew.png" alt="Andrew Perrault"><div class="team__member-info"><h3 class="team__member-name">Andrew Perrault</h3><span class="team__member-role">The Ohio State University</span><p class="team__member-descr">I am an assistant professor in the Department of Computer Science and Engineering at The Ohio State University. My research focuses on multi-agent interactions that arise in combating societal challenges, especially in the areas of conservation and public health. These interactions often involve challenges of uncertainty in the environment and the utility functions of the agents, necessitating approaches that handle scarce data. To achieve this end, I combine methodologies from game theory and multi-agent systems with machine learning, robust planning and optimization techniques.</p></div><ul class="team__member-soc"><li><a class="team__member-icon tw" href="https://twitter.com/PerraultAndrew">Follow me on twitter</a></li><li><a class="team__member-icon gplus" href="https://scholar.google.com/citations?user=2eWscBQAAAAJ&amp;hl=en&amp;oi=ao">My Google+</a></li><li><a class="team__member-icon db" href="https://aperrault.github.io/">Personal website</a></li></ul></div><div class="team__member"><img src="static/images/brandon.png" alt="Brandon Amos"><div class="team__member-info"><h3 class="team__member-name">Brandon Amos</h3><span class="team__member-role">Facebook AI (FAIR)</span><p class="team__member-descr">I am a research scientist at Facebook AI (FAIR) in NYC and study foundational topics in machine learning and optimization, recently involving reinforcement learning, control, optimal transport, and geometry. My research is on learning systems that understand and interact with our world and focuses on integrating structural information and domain knowledge into these systems to represent non-trivial reasoning operations. A key theme of my work in this space involves the use of optimization as a differentiable building block in larger architectures that are end-to-end learned.</p></div><ul class="team__member-soc"><li><a class="team__member-icon tw" href="https://twitter.com/brandondamos">Follow me on twitter</a></li><li><a class="team__member-icon gplus" href="https://scholar.google.com/citations?user=d8gdZR4AAAAJ">My Google+</a></li><li><a class="team__member-icon db" href="http://bamos.github.io/">Personal website</a></li></ul></div></div></section><!-- $Contact section--><section class="contact" id="contact"><div class="contact__inner"><h2 class="contact__title">Contact us</h2><p class="contact__descr">Kai Wang &nbsp | &nbsp <a class="contact__card-item email">kaiwang@g.harvard.edu</a></p></div></section><!-- $Page footer--><footer class="page-footer"><div class="page-footer__inner"><span class="page-footer__copyright">&copy; Copyright 2013 Bak-One | One Page Flat Template</span><a class="page-footer__gotop js-anchor" href="#home">Go to top</a></div></footer><script src="static/js/body.min.js"></script></body></html>